# Transformer from Scratch (PyTorch)

This repository contains a **from-scratch implementation of the Transformer model**
based on the paper **â€œAttention Is All You Needâ€**.

The model is trained on the **English â†’ Hindi** translation task using the
**OPUS-100 (en-hi)** dataset.

---

## Model Implementation Notes

- `model.py` is **written in a simple and explicit manner**
- Code prioritizes **readability and clarity over optimization**
- Variable names and logic are kept easy to follow
- Designed mainly for **study, learning, and understanding Transformers**

This makes the project suitable for:
- Students learning Transformers
- Debugging and experimentation
- Interview preparation

---

## Dataset

- **OPUS-100** Englishâ€“Hindi (`Helsinki-NLP/opus-100`)
- Tokenizers are trained using **BPE**
- Special tokens used:
  - `[PAD]` â€“ padding
  - `[SOS]` â€“ start of sequence
  - `[EOS]` â€“ end of sequence
  - `[UNK]` â€“ unknown token

---

## ğŸ› ï¸ Installation

```bash
pip install -r requirements.txt
